{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Лекция 1. Арифметика чисел с плавающей точкой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## План на сегодня\n",
    "\n",
    "- Арифметика чисел с фиксированной и плавающей точкой\n",
    "- Форматы представления действительных чисел в памяти компьютера\n",
    "- Почему способы хранения чисел в компьютере важны для обучения нейросетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Представление чисел \n",
    "\n",
    "- Действительные числа обозначают различные величины: вероятности, массы, скорости, длины...\n",
    "\n",
    "- Важно знать, как они представляются в компьютере, который оперирует только битами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Представление с фиксированной точкой\n",
    "\n",
    "- Наиболее очевидный формат представления чисел – это формат чисел с **фиксированной точкой**, также известный как **Qm.n** формат\n",
    "\n",
    "- Число **Qm.n** лежит в отрезке $[-(2^m), 2^m - 2^{-n}]$ и его точность $2^{-n}$.\n",
    "\n",
    "- Общий размер памяти для хранения такого числа $m + n + 1$ бит.\n",
    "\n",
    "- Диапазон чисел, представимых таким образом, ограничен."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Числа с плавающей точкой (floating point numbers)\n",
    "\n",
    "Числа в памяти компьютера обычно представляются в виде **чисел с плавающей точкой.**\n",
    "\n",
    "Число с плавающей точкой представляется в виде\n",
    "\n",
    "$$ \\textrm{number} = \\textrm{significand} \\times \\textrm{base}^{\\textrm{exponent}},$$\n",
    "\n",
    "где *significand* – целое число (aka мантисса), *base* – натуральное число (основание) и *exponent* – целое число (может быть отрицательным), например\n",
    "\n",
    "$$ 1.2 = 12 \\cdot 10^{-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Формат фиксированной точки vs формат плавающей точки\n",
    "\n",
    "**Q**: какие достоинства и недостатки у рассмотренных форматов представления действительных чисел?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**A**: они подходят для большинства случаев.\n",
    "\n",
    "- Однако, числа с фиксированной точкой представляют числа из фиксированного интервала и ограничивают **абсолютную** точность.\n",
    "\n",
    "- Числа с плавающей точкой представлют числа с **относительной** точностью и удобны для случаев, когда числа, которые участвуют в вычислениях, имеют различный порядок (например $10^{-1}$ и $10^{5}$).\n",
    "\n",
    "- На практике, если скорость не критически важна, стоит использовать float32 или float64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## IEEE 754\n",
    "В современных компьютерах представление чисел в виде чисел с плавающей точкой регулируется стандартом [IEEE 754](https://en.wikipedia.org/wiki/IEEE_floating_point), который был опубликован в **1985 г.** До этого компьютеры обрабатывали числа с плавающей точкой по-разному!\n",
    "\n",
    "IEEE 754 содержит следующие элементы:\n",
    "- Представление чисел с плавающей точкой (как было описано выше), $(-1)^s \\times c \\times b^q$.\n",
    "- Две бесконечности $+\\infty$ and $-\\infty$\n",
    "- Два типа **NaN**: \"тихий\" NaN (**qNaN**) and сигнализирующий NaN (**sNaN**) \n",
    "    - qNaN не бросает исключение на уровне блока, производящего операции с плавающей точкой, (floating point unit – FPU), до того как вы проверите результат вычислений\n",
    "    - значение sNaN бросает исключение из FPU, если вы используете это значение в вычислениях. Этот тип NaN может быть полезен для инициализиции\n",
    "    - C++11 имеет [стандартный интерфейс](https://en.cppreference.com/w/cpp/numeric/math/nan) для создания разных типов NaN\n",
    "- Правила **округления**\n",
    "- Правила для операций типа $\\frac{0}{0}, \\frac{1}{-0}, \\ldots$\n",
    "\n",
    "Возможные значения определяются через\n",
    "- основание $b$\n",
    "- точность $p$ - число цифр в записи\n",
    "- максимально возможное значение $e_{\\max}$\n",
    "\n",
    "и имеет следующие ограничения\n",
    "\n",
    "- $ 0 \\leq c \\leq b^p - 1$\n",
    "- $1 - e_{\\max} \\leq q + p - 1 \\leq e_{\\max}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Два наиболее используемых формата: single & double\n",
    "\n",
    "Наиболее часто используются следующие форматы: **binary32** и **binary64** (также известные как **single** и **double**).\n",
    "В последнее время популярность набирают вычисления с половинной точностью **binary16**.\n",
    "\n",
    "| Формальное название | Другое название | Основание | Число цифр в записи | Emin | Emax |\n",
    "|------|----------|----------|-------|------|------|\n",
    "|binary16| half precision  | 2 | 11 | -14 | + 15 |  \n",
    "|binary32| single precision | 2 | 24 | -126 | + 127 |  \n",
    "|binary64| double precision | 2|  53|  -1022| +1023|\n",
    "\n",
    "Как выглядит формат двойной точности ```double```\n",
    "\n",
    "<img src=\"./double64.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Примеры\n",
    "\n",
    "- Для числа +0\n",
    "    - *sign* - 0\n",
    "    - *exponent* - 00000000000\n",
    "    - *fraction* - все нули\n",
    "- Для числа -0\n",
    "    - *sign* - 1\n",
    "    - *exponent* - 00000000000\n",
    "    - *fraction* - все нули\n",
    "- Для +infinity\n",
    "    - *sign* - 0\n",
    "    - *exponent* - 11111111111\n",
    "    - *fraction* - все нули\n",
    "\n",
    "**Q**: Как будет выглядеть -infinity и NaN ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Точность и размер памяти \n",
    "\n",
    "**Относительная точность** для различных форматов\n",
    "\n",
    "- половинная точность или float16: $10^{-3} - 10^{-4}$,\n",
    "- одинарная точность или float32: $10^{-7}-10^{-8}$,\n",
    "- двойная точность или float64: $10^{-14}-10^{-16}$.\n",
    "\n",
    "<font color='red'> Crucial note 1: </font> **float16** занимает **2 байта**, **float32** занимает **4 байта**, **float64** занимает **8 байт**\n",
    "\n",
    "<font color='red'> Crucial note 2: </font> Обычно в \"железе\" поддерживается одинарная и двойная точность. Для обучения нейросетей становится популярным использовать половинную точность или даже меньше... Подробности [тут](https://arxiv.org/pdf/1905.12334.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Альтернатива стандарту IEEE 754\n",
    "\n",
    "Недостатки IEEE 754:\n",
    "- переполнения до бесконечности или нуля\n",
    "- много разных NaN\n",
    "- невидимые ошибки округления (об этом ниже)\n",
    "- точность либо очень хорошая, либо очень плохая\n",
    "- субнормальные числа – числа между 0 и минимально возможным представимым числом, то есть мантисса субнормального числа начинается с ведущего 0\n",
    "\n",
    "Концепция posits может заменить числа с плавающей точкой, см [статью](http://www.johngustafson.net/pdfs/BeatingFloatingPoint.pdf)\n",
    "\n",
    "<img width=600 src=\"./posit.png\">\n",
    "\n",
    "- выражают числа с некоторой точностью, но указывают предел изменения\n",
    "- отсутствуют переполнения до бесконечности или нуля\n",
    "- пример хранения числа\n",
    "\n",
    "<img width=600 src=\"./posit_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Пример 1: потеря точности при делении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9259246\n",
      "0.1040364727377892\n",
      "-5.9604645e-08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "#c = random.random()\n",
    "#print(c)\n",
    "c = np.float32(0.925924589693)\n",
    "print(c)\n",
    "a = np.float32(8.9)\n",
    "b = np.float32(c / a)\n",
    "print('{0:10.16f}'.format(b))\n",
    "print(a * b - c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Пример 2: потеря точности при извлечении корня"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0000000293644121\n",
      "1.468220602873771e-07\n"
     ]
    }
   ],
   "source": [
    "#a = np.array(1.585858585887575775757575e-5, dtype=np.float)\n",
    "a = np.float32(5.0)\n",
    "b = np.sqrt(a)\n",
    "print('{0:10.16f}'.format((b ** 2 - a) / a))\n",
    "print((b ** 2 - a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Пример 3: потеря точности при вычислении экспоненты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = np.array(2.2882727271099, dtype=np.float16)\n",
    "b = np.log(a)\n",
    "print(np.exp(b) - a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Выводы\n",
    "\n",
    "- Для некоторых чисел обратные функции дают неточный ответ\n",
    "- Относительная точность должна сохраняться в соответствии со стандартом IEEE\n",
    "- Это требование не выполняется на многих современных GPU\n",
    "- Подробный анализ выполнимости стандарта IEEE 754 на GPU от NVIDIA можно найти [здесь](https://docs.nvidia.com/cuda/floating-point/index.html#considerations-for-heterogeneous-world) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Потеря значимых цифр\n",
    "\n",
    "- Многие операции приводят к потере значимых цифр в результате, этот эффект называется [loss of significance](https://en.wikipedia.org/wiki/Loss_of_significance)\n",
    "- Например, при вычитании двух близких больших чисел результат будет иметь меньше правильных цифр, чем исходные цифры\n",
    "- Это связано с алгоритмами и их свойствами (прямой/обратной устойчивости), которые мы обсудим далее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123456789999999.11\n",
      "123456789999998.1\n",
      "1.01562500000000000\n"
     ]
    }
   ],
   "source": [
    "a = np.float64(123456789999999.11)\n",
    "print(a)\n",
    "b = np.float64(123456789999998.1)\n",
    "print(b)\n",
    "c = a - b\n",
    "print('{0:10.17f}'.format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Алгоритм суммирования\n",
    "\n",
    "Однако ошибка округления зависит от алгоритма вычисления.\n",
    "\n",
    "- Рассмотрим простейшую задачу: дано $n$ чисел с плавающей точкой $x_1, \\ldots, x_n$  \n",
    "\n",
    "- Необходимо вычислить их сумму\n",
    "\n",
    "$$ S = \\sum_{i=1}^n x_i = x_1 + \\ldots + x_n.$$\n",
    "\n",
    "- Простейший алгоритм: складывать числа $x_1, \\ldots, x_n$ одно за другим\n",
    "\n",
    "- Какова ошибка такого алгоритма при работе в неточной арифметике?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Наивный алгоритм\n",
    "\n",
    "Сложим числа одно за другим: \n",
    "\n",
    "$$y_1 = x_1, \\quad y_2 = y_1 + x_2, \\quad y_3 = y_2 + x_3, \\ldots.$$\n",
    "\n",
    "- В **худшем случае** ошибка пропорциональна $\\mathcal{O}(n)$, в то время как **средне-квадратичная** ошибка $\\mathcal{O}(\\sqrt{n})$.\n",
    "\n",
    "- **Алгоритм Кахана** (Kahan algorithm) даёт ошибку в худшем случае $\\mathcal{O}(1)$ (то есть она не зависит от $n$).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Алгоритм Кахана (Kahan summation)\n",
    "\n",
    "Следующий алгоритм даёт ошибку $2 \\varepsilon + \\mathcal{O}(n \\varepsilon^2)$, где $\\varepsilon$ – машинная точность.\n",
    "```python\n",
    "s = 0\n",
    "c = 0\n",
    "for i in range(len(x)):\n",
    "    y = x[i] - c\n",
    "    t = s + y\n",
    "    c = (t - s) - y\n",
    "    s = t\n",
    "```\n",
    "\n",
    "- Пример использования такого метода в библиотеке глубокого обучения PyTorch - https://github.com/pytorch/torchdistx/pull/52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in np sum: -2.4e-07\n",
      "Error in Kahan sum Numba: 1.7e-08\n",
      "Error in Kahan sum JAX: 0.0e+00\n",
      "Error in dumb sum: -1.0e-04\n",
      "Error in math fsum: 1.3e-12\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import jax\n",
    "from numba import jit as numba_jit\n",
    "\n",
    "n = 10 ** 6\n",
    "sm = 1e-10\n",
    "x = jnp.ones(n, dtype=jnp.float32) * sm\n",
    "x = x.at[0].set(1.)\n",
    "true_sum = 1.0 + (n - 1)*sm\n",
    "approx_sum = jnp.sum(x)\n",
    "math_fsum = math.fsum(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def dumb_sum(x):\n",
    "    s = jnp.float32(0.0)\n",
    "    def b_fun(i, val):\n",
    "        return val + x[i] \n",
    "    s = jax.lax.fori_loop(0, len(x), b_fun, s)\n",
    "    return s\n",
    "\n",
    "\n",
    "@numba_jit(nopython=True)\n",
    "def kahan_sum_numba(x):\n",
    "    s = np.float32(0.0)\n",
    "    c = np.float32(0.0)\n",
    "    for i in range(len(x)):\n",
    "        y = x[i] - c\n",
    "        t = s + y\n",
    "        c = (t - s) - y\n",
    "        s = t\n",
    "    return s\n",
    "\n",
    "@jax.jit\n",
    "def kahan_sum_jax(x):\n",
    "    s = jnp.float32(0.0)\n",
    "    c = jnp.float32(0.0)\n",
    "    def b_fun2(i, val):\n",
    "        s, c = val\n",
    "        y = x[i] - c\n",
    "        t = s + y\n",
    "        c = (t - s) - y\n",
    "        s = t\n",
    "        return s, c\n",
    "    s, c = jax.lax.fori_loop(0, len(x), b_fun2, (s, c))\n",
    "    return s\n",
    "\n",
    "k_sum_numba = kahan_sum_numba(np.array(x))\n",
    "k_sum_jax = kahan_sum_jax(x)\n",
    "d_sum = dumb_sum(x)\n",
    "print('Error in np sum: {0:3.1e}'.format(approx_sum - true_sum))\n",
    "print('Error in Kahan sum Numba: {0:3.1e}'.format(k_sum_numba - true_sum))\n",
    "print('Error in Kahan sum JAX: {0:3.1e}'.format(k_sum_jax - true_sum))\n",
    "print('Error in dumb sum: {0:3.1e}'.format(d_sum - true_sum))\n",
    "print('Error in math fsum: {0:3.1e}'.format(math_fsum - true_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ещё один пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_list = [1, 1e20, 1, -1e20]\n",
    "print(math.fsum(test_list))\n",
    "print(np.sum(test_list))\n",
    "print(1 + 1e20 + 1 - 1e20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Выводы по операциям с числами с плавающей точкой\n",
    "\n",
    "- Существуют различные форматы и подходы к хранению чисел в памяти\n",
    "- Операции с числами с плавающей точкой могут приводить к неточным ответам из-за ошибок округления - требуется осторожность!\n",
    "- Для многих стандартных алгоритмов вопросы устойчивости хорошо изучены и проблемы, связанные с этим, легко обнаруживаются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Кратко про нейросети\n",
    "\n",
    "- Элементарные функции могут быть параметризованы, например $f(x) = Wx + b$ – линейная функция с параметрами $W$ и $b$\n",
    "- Нейросеть – это суперпозиция параметрических функций\n",
    "- Параметры элементарных функций, из которых состоит нейросеть, задаются с помощью чисел с плавающей точкой\n",
    "- Формат хранения параметров напрямую влияет на то, насколько точно и устойчиво будет вычисляться итоговый результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Форматы представления чисел и нейронные сети\n",
    "\n",
    "- Для быстрой и энергоэффективной работы с нейросетями разработаны специальные форматы хранения чисел (подробности далее)\n",
    "- Они помогают ускорить получение обученных моделей с некоторой потерей точности промежуточных вычислений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Влияние формата представления чисел на обучение нейросетей\n",
    "\n",
    "- Веса в слоях (полносвязном, свёрточном, действие функций активации) могут храниться с различной точностью\n",
    "- Это важно для повышении энергоэффективности устройств, на которых запускается обученная нейросеть\n",
    "- Проект [DeepFloat](https://github.com/facebookresearch/deepfloat) от Facebook показывает, как переизобрести операции с плавающей точкой, чтобы они были эффективны для обучения нейросетей, подробности см. в [статье](https://arxiv.org/pdf/1811.01721.pdf)\n",
    "- Влияние формата представления чисел на значения градиентов активаций\n",
    "\n",
    "<img width=500, src=\"./grad_norm_fp16.png\">\n",
    "\n",
    "- И на кривые обучения\n",
    "\n",
    "<img width=500, src=\"./train_val_curves.png\">\n",
    "\n",
    "Графики взяты из [этой статьи](https://arxiv.org/pdf/1710.03740.pdf%EF%BC%89%E3%80%82)\n",
    "\n",
    "- Проект [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) предлагает ядра для CUDA, где реализованы базовые операции с малобитным представлением чисел"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## bfloat16 (Brain Floating Point)\n",
    "\n",
    "- Этот формат требует 16 битов\n",
    "    - 1 бит для знака\n",
    "    - 8 битов для экспоненты\n",
    "    - 7 bits для мантиссы\n",
    "    <img src=\"./bfloat16.png\">\n",
    "- Усечённая одинарная точность из стандарта IEEE\n",
    "- Какая разница между float32 и float16?\n",
    "- Этот формат используется в Intel FPGA, Google TPU, Xeon CPUs и других платформах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensor Float от Nvidia ([блог об этом формате](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/))\n",
    "\n",
    "- Сравнение с другими форматами\n",
    "\n",
    "<img src=\"./tensor_float_cf.png\">\n",
    "\n",
    "- Результаты\n",
    "\n",
    "<img src=\"./TF32-BERT.png\">\n",
    "\n",
    "- PyTorch и Tensorflow, поддерживающие этот формат, доступны в [Nvidia NGC](https://ngc.nvidia.com/catalog/all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Смешанная точность ([документация от Nvidia](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html))\n",
    "\n",
    "- Основная идея:\n",
    "    - Поддерживать копии весов в одинарной точности\n",
    "    - Тогда на каждой итерации\n",
    "        - Сделать копию весов в половинной точности\n",
    "        - Сделать проход вперёд с весами в половинной точности\n",
    "        - Умножить значение функции ошибки на множитель $S$\n",
    "        - Вычислить градиент в половинной точности\n",
    "        - Умножить градиенты весов на $1/S$\n",
    "        - Обновить веса \n",
    "    - Множитель $S$ это гиперпараметр\n",
    "    - Постоянный: такое значение, что его произведение на максимальное абсолютное значение градиента меньше чем 65504 (максимальное число, представимое в половинной точности).\n",
    "    - Динамическое обновление, основанное на статистиках текущих градиентов\n",
    "- Сравнение производительности\n",
    "<img src=\"./mixed_precision_res.png\" width=500>\n",
    "\n",
    "- Существуют расширения для автоматического включения этой опции, больше подробностей [тут](https://developer.nvidia.com/automatic-mixed-precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Резюме\n",
    "\n",
    "- Числа с плавающей точкой – основа для проведения вычислений\n",
    "- Использование нейросетей поставило новые вопросы и предъявило новые требования к числам и операциям с ними на уровне железа\n",
    "- Помимо использования фиксированнго формата применяют смешанную точность"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
