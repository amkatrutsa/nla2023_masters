{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0871b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Лекция 7. Матричные функции, часть 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1776a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Простейшая матричная функция: матричный полином\n",
    "\n",
    "Матричный полином имеет очень простой вид\n",
    "\n",
    "$$ P(A) = \\sum_{k=0}^n c_k A^k. $$\n",
    "\n",
    "[Теорема Гамильтона-Кэли](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%93%D0%B0%D0%BC%D0%B8%D0%BB%D1%8C%D1%82%D0%BE%D0%BD%D0%B0_%E2%80%94_%D0%9A%D1%8D%D0%BB%D0%B8) утверждает, что $F(A) = 0$ где $F(\\lambda) = \\det(A - \\lambda I)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb81651",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Матричный полином как способ построения любой матричной фугнкции\n",
    "\n",
    "Можно определить функцию от матрицы с помощью ряда Тейлора:  \n",
    "\n",
    "$$ f(A) = \\sum_{k=0}^{\\infty} c_k A^k. $$\n",
    "\n",
    "Сходимость означает как сходимость в некоторой **матричной норме**.  \n",
    "\n",
    "Примером такого ряда является ряд Неймана\n",
    "\n",
    "$$ (I - F)^{-1} = \\sum_{k=0}^{\\infty} F^k, $$\n",
    "\n",
    "который определён для $\\rho(F) < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad0f47d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ряд для матричной экспоненты\n",
    "\n",
    "Наиболее известной матричной функцией является **матричная экспонента**. В скалярном случае ряд выглядит следующим образом  \n",
    "\n",
    "$$ e^x = 1 + x + \\frac{x^2}{2} + \\frac{x^3}{6} + \\ldots = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!}, $$\n",
    "\n",
    "и он напрямую обобщается на матричный случай:  \n",
    "\n",
    "$$ e^A = \\sum_{k=0}^{\\infty} \\frac{A^k}{k!}. $$\n",
    "\n",
    "Этот ряд всегда сходится, так как выполнено следующее равенство\n",
    "\n",
    "$$\\sum_{k=0}^{\\infty} \\frac{\\Vert A \\Vert^k}{k!} = e^{\\Vert A \\Vert}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb67a6d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Почему матричная экспонента важна?\n",
    "\n",
    "Огромное количество прикладных задач сводится к системе обыкновенных дифференциальных уравнений вида\n",
    "\n",
    "$$ \\frac{dy}{dt} = Ay, \\quad y(0) = y_0. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687b3c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Обыкновенные дифференциальные уравнения и матричная экспонента\n",
    "\n",
    "- Дано уравнение\n",
    "\n",
    "$$\\frac{dy}{dt} = Ay, \\quad y(0) = y_0.$$\n",
    "\n",
    "- Формально решение задаётся выражением $y(t) = e^{At} y_0$, поэтому если известна $e^{At}$ (или мы можем быстро умножить матричную экспоненту на вектор), то решение можно получить гораздо быстрее по сравнению с методами, основанными на шагах по времени  \n",
    "- Действительно,\n",
    "\n",
    "$$\\frac{d}{dt} e^{At} = \\frac{d}{dt} \\sum_{k=0}^{\\infty} \\frac{t^k A^k}{k!} = \\sum_{k=1}^{\\infty} \\frac{t^{k-1} A^{k}}{(k-1)!}  = A e^{At}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dfb835",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Матричная экспонента и шаги по времени\n",
    "\n",
    "Матричная экспонента может быть гораздо лучше, чем решение с помощью, например, схемы Эйлера:\n",
    "\n",
    "$$\\frac{dy}{dt} \\approx \\frac{y_{k+1} - y_k}{\\tau} = A y_k, \\quad y_{k+1} = y_k + \\tau A y_k,$$\n",
    "\n",
    "если мы знаем как вычислить произведение матричной экспоненты на вектор, используя только произведения матрицы $A$ на вектор.\n",
    "\n",
    "Для плотных матриц матричная экспонента даёт **точный** ответ для ОДУ в любой момент времени $t$ по сравнению с приближённым решением, полученным из схемы Эйлера или схожих подходов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b161860",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Как вычислять матричные функции, включая матричную экспоненту?\n",
    "\n",
    "- Существует очень много методов даже для матричной экспоненты!\n",
    "\n",
    "- См. статью [C. Van Loan, C. Moler, Nineteen Dubious Ways to Compute the Exponential of a Matrix, Twenty-Five Years Later](http://www.cs.cornell.edu/cv/researchpdf/19ways+.pdf)\n",
    "\n",
    "- Самый простой метод – это диагонализация матрицы:  \n",
    "\n",
    "$$ A = S \\Lambda S^{-1}, $$\n",
    "\n",
    "где столбцы $S$ – собственные векторы матрицы $A$, тогда\n",
    "\n",
    "$$ F(A) = S F(\\Lambda) S^{-1}. $$\n",
    "\n",
    "**Проблема: диагонализация неустойчива!** (и не любая матрица диагонализуема)\n",
    "\n",
    "Далее короткое демо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a450a526",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.e+00 1.e+00 0.e+00 0.e+00]\n",
      " [0.e+00 1.e+00 1.e+00 0.e+00]\n",
      " [0.e+00 0.e+00 1.e+00 1.e+00]\n",
      " [0.e+00 0.e+00 1.e-04 1.e+00]]\n",
      "10000.000049999939\n",
      "[[ 1.00000000e+00  0.00000000e+00 -7.59220793e-13  1.00000000e+04]\n",
      " [ 0.00000000e+00  1.00000000e+00  1.00000000e+00 -4.19097369e-13]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.00000000e-04  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "eps = 1e-4\n",
    "p = 4\n",
    "a = np.eye(p)\n",
    "for i in range(p-1):\n",
    "    a[i, i+1] = 1\n",
    "    \n",
    "a[p-1, 2] = eps\n",
    "print(a)\n",
    "val, vec = np.linalg.eig(a)\n",
    "#print a\n",
    "print(np.linalg.norm(a - vec.dot(val[:, np.newaxis] * np.linalg.inv(vec))))\n",
    "#print 'S * D * S^{-1}:' \n",
    "print(vec.dot(val[:, np.newaxis] * np.linalg.inv(vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49e568",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Сейчас мы вычислим матричную экспоненту с помощью диагонализации от **возмущённой Жордановой клетки**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51402285",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "Difference = 5.959978842992802\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "eps = 1e-16\n",
    "p = 5\n",
    "a = np.eye(p)\n",
    "for i in range(p-1):\n",
    "    a[i, i+1] = 1\n",
    "    \n",
    "a[p-1, 0] = eps\n",
    "a = np.array(a)\n",
    "val, vec = np.linalg.eig(a)\n",
    "print(np.linalg.norm(a - vec.dot(np.diag(val)).dot(np.linalg.inv(vec))))\n",
    "\n",
    "fun = lambda x: np.exp(x)\n",
    "\n",
    "#Using diagonalization\n",
    "fun_diag = vec.dot(np.diag(fun(val))).dot(np.linalg.inv(vec))\n",
    "\n",
    "\n",
    "#Using Schur\n",
    "import scipy.linalg\n",
    "fun_m = scipy.linalg.expm(a)\n",
    "print('Difference = {}'.format(np.linalg.norm(fun_m - fun_diag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a2812",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Как работает функция ```funm```?\n",
    "\n",
    "- Матричная экспонента – это особая функция, и для её вычисления существуют специальные методы.  \n",
    "\n",
    "- Для произвольной функции $F$, есть замечательный **алгоритм Шура-Парлетта**, который основан для **теореме Шура**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1921ffb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Алгоритм Шура-Парлетта\n",
    "\n",
    "- Для данной матрицы $A$ мы хотим вычислить $F(A)$, и можем вычислить $F$ только в **скалярных точках**\n",
    "\n",
    "- Сначала сведём матрицу $A$ к **треугольной форме**  \n",
    "\n",
    "$$ A = U T U^*. $$\n",
    "\n",
    "- Поэтому  $F(A)=U F(T) U^*$\n",
    "\n",
    "- Нам осталось вычислить функцию от треугольной матрицы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a833260",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Вычисление функции от треугольных матриц\n",
    "\n",
    "- Мы знаем значения на диагоналях\n",
    "\n",
    "$$ F_{ii} = F(T_{ii}), $$\n",
    "\n",
    "также мы знаем, что\n",
    "\n",
    "$$ F T = T F, $$\n",
    "\n",
    "то есть значение матричной функции **коммутирует** с самой матрицей. \n",
    "\n",
    "- Матричная функция от треугольной матрицы есть треугольная матрица.\n",
    "- Используя известные значения на главной диагонали и свойство коммутативности, мы получим последовательно остальные диагонали:\n",
    "\n",
    "$$f_{ij} = t_{ij} \\frac{f_{ii} - f_{jj}}{t_{ii} - t_{jj}} + \\sum_{k=i+1}^{j-1} \\frac{f_{ik} t_{kj} - t_{ki}f_{kj}}{t_{ii} - t_{jj}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27755420",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Матричные функции: ещё одно определение\n",
    "\n",
    "- Одним из способов определение матричной функции $f(A)$ является использование **канонической формы Жордана**.\n",
    "\n",
    "- Более элегантный способ определить матричные функции – это использовать **интегральное представление Коши:**\n",
    "\n",
    "$$\n",
    "    f(A) = \\frac{1}{2\\pi i} \\int_{\\Gamma} f(z) (zI - A)^{-1} dz,\n",
    "$$\n",
    "\n",
    "где $f(z)$ аналитическая функция на границе и внутри замкнутого контура $\\Gamma$, который покрывает спектр матрицы $A$.\n",
    "\n",
    "- Определение можно обобщить на случай **операторов**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5f40a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Важные матричные функции\n",
    "\n",
    "- Матричная экспонента используется для решения ОДУ $\\frac{dy}{dt} = Ay$ в явном виде $y = e^{At}y_0.$\n",
    "- $\\cos(A), \\sin(A)$ используются для решения волнового уравнения $\\frac{d^2 y}{dt^2} + Ay = 0.$\n",
    "- Функция знака, $\\mathrm{sign}(A)$, используется для вычисления **спектральных проекций.**\n",
    "- Обратный квадратный корень из матрицы $A^{-1/2}$ необходим в различных задачах, например, для генерирования сэмплов из нормального распределения"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
